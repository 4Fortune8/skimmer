{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a6638a5e-01f6-4265-bed8-94c93fe1426b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "12bc8903-bf1d-4299-9f01-c6a6171f864d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import scrapy\n",
    "import csv\n",
    "from scrapy.crawler import CrawlerProcess\n",
    "from bs4 import BeautifulSoup, Tag\n",
    "import time\n",
    "def print_css_tree(html):\n",
    "    soup = BeautifulSoup(html, 'html.parser')\n",
    "\n",
    "    def recursive_print(tag, prefix=\".\"):\n",
    "        if isinstance(tag, Tag):  # Check if tag is a BeautifulSoup Tag object\n",
    "            class_str = \".\".join(tag.get('class', []))\n",
    "            print(f\"{prefix}{tag.name}.{class_str}\")\n",
    "            prefix += \"  \"\n",
    "            for child in tag.children:\n",
    "                recursive_print(child, prefix)\n",
    "                \n",
    "    recursive_print(soup)\n",
    "\n",
    "\n",
    "\n",
    "def checkProfile(row,profiles):\n",
    "    try:\n",
    "        for sublist in profiles:\n",
    "            if row in sublist:\n",
    "                return True\n",
    "        return False\n",
    "    except:\n",
    "        return False\n",
    "\n",
    "def convert_to_number(s):\n",
    "            multipliers = {'K': 1000, 'M': 1000000, 'B': 1000000000}\n",
    "            # Check the last character of the string\n",
    "            if s[-1] in multipliers:\n",
    "                # If the last character is a multiplier, remove it from the string,\n",
    "                # convert the remaining string to a float, multiply by the appropriate value,\n",
    "                # and convert the result to an integer\n",
    "                return int(float(s[:-1]) * multipliers[s[-1]])\n",
    "            else:\n",
    "                # If the last character is not a multiplier, just convert the string to an integer\n",
    "                return int(s)\n",
    "            \n",
    "            \n",
    "def extract_values(text):\n",
    "    # Find the index of the item with '%'\n",
    "    index = next((i for i, s in enumerate(text) if '%' in s), None)\n",
    "    if index is not None and index > 0:\n",
    "        # If found and it's not the first item, return the item and the one before it\n",
    "        return [text[index - 1], text[index]]\n",
    "    else:\n",
    "        # If not found or it's the first item, return None\n",
    "        return [None, None]\n",
    "\n",
    "# Usage:            \n",
    "class BlogSpider(scrapy.Spider):  \n",
    "    \n",
    "    def __init__(self):\n",
    "  \n",
    "        self.name = 'blogspider'\n",
    "        self.base= 'https://socialblade.com/youtube/c/@'\n",
    "        self.start_urls =[]\n",
    "        self.profiles = []\n",
    "        \n",
    "        with open('data\\profiles\\profiles2024-5.csv', 'r', encoding='utf-8') as file:\n",
    "            reader = csv.reader(file)\n",
    "            profiles = list(reader)\n",
    "\n",
    "\n",
    "        with open('data\\output2024-05-09210054.csv', 'r', encoding='utf-8') as file:\n",
    "            reader = csv.reader(file)\n",
    "            data = list(reader)\n",
    "            print(data[0])\n",
    "            for row in data[1:3]:\n",
    "                print(row)\n",
    "                booler = checkProfile(row[4],profiles)\n",
    "                if not booler:\n",
    "                    self.start_urls.append(self.base+(row[4]))\n",
    "\n",
    "    \n",
    "    def parse(self, response):\n",
    "        time.sleep(10)\n",
    "        # Extract data from the new CSS selector\n",
    "        for element in response.css('div.YouTubeUserTopInfo:nth-child(3) > span:nth-child(3)'):\n",
    "            subscribers = element.css('::text').extract_first().strip()\n",
    "            \n",
    "            subscribers = convert_to_number(subscribers.strip())\n",
    "        try:\n",
    "            for title in response.css('#socialblade-user-content > div:nth-child(3) > div'):\n",
    "                text=  title.css('::text').extract()\n",
    "                data= []\n",
    "                text = extract_values(text)\n",
    "                text[0] = text[0].strip()  # Remove leading and trailing whitespace\n",
    "                newsubscribers = convert_to_number(text[0])\n",
    "                color= title.css('sup>span::attr(style)').extract()\n",
    "                currentAccount= response.request.meta['redirect_urls'][0][len(self.base):]\n",
    "                text[1]= text[1][:-1]\n",
    "                if color[0] == \"color:#e53b00;\":\n",
    "                    text[1]='-'+text[1]\n",
    "                    print(currentAccount,subscribers,newsubscribers,text[1])\n",
    "                    data.append((currentAccount,subscribers,newsubscribers,text[1]))\n",
    "                else:\n",
    "                    data.append((currentAccount,subscribers,newsubscribers,text[1]))\n",
    "                yield self.addProfile([currentAccount,subscribers,newsubscribers,text[1]])\n",
    "        except:\n",
    "            currentAccount= response.request.meta['redirect_urls'][0][len(self.base):]\n",
    "            yield self.addProfile([currentAccount,subscribers,0,0])\n",
    "    \n",
    "    \n",
    "    def addProfile(self, profile):\n",
    "        with open('data\\profiles\\profiles2024-5.csv', 'a',newline='', encoding='utf-8') as file:\n",
    "            writer = csv.writer(file)\n",
    "            writer.writerow([profile[0],profile[1],profile[2],profile[3]])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "c571d5d7-61e7-416d-b79d-332194d8a26f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-05-09 21:12:09 [scrapy.utils.log] INFO: Scrapy 2.11.1 started (bot: scrapybot)\n",
      "2024-05-09 21:12:09 [scrapy.utils.log] INFO: Versions: lxml 5.2.1.0, libxml2 2.11.7, cssselect 1.2.0, parsel 1.9.1, w3lib 2.1.2, Twisted 24.3.0, Python 3.10.11 (tags/v3.10.11:7d4cc5a, Apr  5 2023, 00:38:17) [MSC v.1929 64 bit (AMD64)], pyOpenSSL 24.1.0 (OpenSSL 3.2.1 30 Jan 2024), cryptography 42.0.7, Platform Windows-10-10.0.22631-SP0\n",
      "2024-05-09 21:12:09 [scrapy.addons] INFO: Enabled addons:\n",
      "[]\n",
      "2024-05-09 21:12:09 [scrapy.utils.log] DEBUG: Using reactor: twisted.internet.selectreactor.SelectReactor\n",
      "2024-05-09 21:12:09 [scrapy.extensions.telnet] INFO: Telnet Password: 3e2b63ea5aa494b4\n",
      "2024-05-09 21:12:09 [scrapy.middleware] INFO: Enabled extensions:\n",
      "['scrapy.extensions.corestats.CoreStats',\n",
      " 'scrapy.extensions.telnet.TelnetConsole',\n",
      " 'scrapy.extensions.logstats.LogStats']\n",
      "2024-05-09 21:12:09 [scrapy.crawler] INFO: Overridden settings:\n",
      "{'USER_AGENT': 'Mozilla/4.0 (compatible; MSIE 7.0; Windows NT 5.1)'}\n",
      "2024-05-09 21:12:09 [scrapy.middleware] INFO: Enabled downloader middlewares:\n",
      "['scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',\n",
      " 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',\n",
      " 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',\n",
      " 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',\n",
      " 'scrapy.downloadermiddlewares.retry.RetryMiddleware',\n",
      " 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',\n",
      " 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',\n",
      " 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',\n",
      " 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',\n",
      " 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',\n",
      " 'scrapy.downloadermiddlewares.stats.DownloaderStats']\n",
      "2024-05-09 21:12:09 [scrapy.middleware] INFO: Enabled spider middlewares:\n",
      "['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',\n",
      " 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',\n",
      " 'scrapy.spidermiddlewares.referer.RefererMiddleware',\n",
      " 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',\n",
      " 'scrapy.spidermiddlewares.depth.DepthMiddleware']\n",
      "2024-05-09 21:12:09 [scrapy.middleware] INFO: Enabled item pipelines:\n",
      "[]\n",
      "2024-05-09 21:12:09 [scrapy.core.engine] INFO: Spider opened\n",
      "2024-05-09 21:12:09 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)\n",
      "2024-05-09 21:12:09 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6026\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['video_name', 'chanel_display_name', 'views', 'age', 'chanel_id']\n",
      "['I Built a Theme Park In My House!', 'Ben Azelart', '18M views', '11 days ago', 'BenAzelart']\n",
      "[\"New items Claire's 2021 ! Elsa & Anna toddlers are shopping - Barbie\", 'Come Play With Me', '77M views', '2 years ago', 'ComePlayWithMe']\n"
     ]
    },
    {
     "ename": "ReactorNotRestartable",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mReactorNotRestartable\u001b[0m                     Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[33], line 7\u001b[0m\n\u001b[0;32m      2\u001b[0m process \u001b[38;5;241m=\u001b[39m CrawlerProcess({\n\u001b[0;32m      3\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mUSER_AGENT\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mMozilla/4.0 (compatible; MSIE 7.0; Windows NT 5.1)\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m      4\u001b[0m })\n\u001b[0;32m      6\u001b[0m process\u001b[38;5;241m.\u001b[39mcrawl(BlogSpider)\n\u001b[1;32m----> 7\u001b[0m \u001b[43mprocess\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstart\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;66;03m# the script will block here until the crawling is finished\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\scrapy\\crawler.py:429\u001b[0m, in \u001b[0;36mCrawlerProcess.start\u001b[1;34m(self, stop_after_crawl, install_signal_handlers)\u001b[0m\n\u001b[0;32m    425\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m install_signal_handlers:\n\u001b[0;32m    426\u001b[0m     reactor\u001b[38;5;241m.\u001b[39maddSystemEventTrigger(\n\u001b[0;32m    427\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mafter\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstartup\u001b[39m\u001b[38;5;124m\"\u001b[39m, install_shutdown_handlers, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_signal_shutdown\n\u001b[0;32m    428\u001b[0m     )\n\u001b[1;32m--> 429\u001b[0m \u001b[43mreactor\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43minstallSignalHandlers\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minstall_signal_handlers\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\twisted\\internet\\base.py:693\u001b[0m, in \u001b[0;36mReactorBase.run\u001b[1;34m(self, installSignalHandlers)\u001b[0m\n\u001b[0;32m    692\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mrun\u001b[39m(\u001b[38;5;28mself\u001b[39m, installSignalHandlers: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m--> 693\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstartRunning\u001b[49m\u001b[43m(\u001b[49m\u001b[43minstallSignalHandlers\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minstallSignalHandlers\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    694\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    695\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmainLoop()\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\twisted\\internet\\base.py:930\u001b[0m, in \u001b[0;36mReactorBase.startRunning\u001b[1;34m(self, installSignalHandlers)\u001b[0m\n\u001b[0;32m    928\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m error\u001b[38;5;241m.\u001b[39mReactorAlreadyRunning()\n\u001b[0;32m    929\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_startedBefore:\n\u001b[1;32m--> 930\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m error\u001b[38;5;241m.\u001b[39mReactorNotRestartable()\n\u001b[0;32m    932\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_signals\u001b[38;5;241m.\u001b[39muninstall()\n\u001b[0;32m    933\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_installSignalHandlers \u001b[38;5;241m=\u001b[39m installSignalHandlers\n",
      "\u001b[1;31mReactorNotRestartable\u001b[0m: "
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    process = CrawlerProcess({\n",
    "        'USER_AGENT': 'Mozilla/4.0 (compatible; MSIE 7.0; Windows NT 5.1)'\n",
    "    })\n",
    "\n",
    "    process.crawl(BlogSpider)\n",
    "    process.start() # the script will block here until the crawling is finished\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3f90b31-9097-485e-bb28-0d015feacf1c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
