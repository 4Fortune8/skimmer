{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a6638a5e-01f6-4265-bed8-94c93fe1426b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "12bc8903-bf1d-4299-9f01-c6a6171f864d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import scrapy\n",
    "import csv\n",
    "from scrapy.crawler import CrawlerProcess\n",
    "from bs4 import BeautifulSoup, Tag\n",
    "import time\n",
    "def print_css_tree(html):\n",
    "    soup = BeautifulSoup(html, 'html.parser')\n",
    "\n",
    "    def recursive_print(tag, prefix=\".\"):\n",
    "        if isinstance(tag, Tag):  # Check if tag is a BeautifulSoup Tag object\n",
    "            class_str = \".\".join(tag.get('class', []))\n",
    "            print(f\"{prefix}{tag.name}.{class_str}\")\n",
    "            prefix += \"  \"\n",
    "            for child in tag.children:\n",
    "                recursive_print(child, prefix)\n",
    "                \n",
    "    recursive_print(soup)\n",
    "\n",
    "\n",
    "\n",
    "def checkProfile(row,profiles):\n",
    "    try:\n",
    "        for sublist in profiles:\n",
    "            if row in sublist:\n",
    "                return True\n",
    "        return False\n",
    "    except:\n",
    "        return False\n",
    "\n",
    "def convert_to_number(s):\n",
    "            multipliers = {'K': 1000, 'M': 1000000, 'B': 1000000000}\n",
    "            # Check the last character of the string\n",
    "            if s[-1] in multipliers:\n",
    "                # If the last character is a multiplier, remove it from the string,\n",
    "                # convert the remaining string to a float, multiply by the appropriate value,\n",
    "                # and convert the result to an integer\n",
    "                return int(float(s[:-1]) * multipliers[s[-1]])\n",
    "            else:\n",
    "                # If the last character is not a multiplier, just convert the string to an integer\n",
    "                return int(s)\n",
    "            \n",
    "            \n",
    "def extract_values(text):\n",
    "    # Find the index of the item with '%'\n",
    "    index = next((i for i, s in enumerate(text) if '%' in s), None)\n",
    "    if index is not None and index > 0:\n",
    "        # If found and it's not the first item, return the item and the one before it\n",
    "        return [text[index - 1], text[index]]\n",
    "    else:\n",
    "        # If not found or it's the first item, return None\n",
    "        return [None, None]\n",
    "\n",
    "# Usage:            \n",
    "class BlogSpider(scrapy.Spider):  \n",
    "    \n",
    "    def __init__(self):\n",
    "  \n",
    "        self.name = 'blogspider'\n",
    "        self.base= 'https://socialblade.com/youtube/c/@'\n",
    "        self.start_urls =[]\n",
    "        self.profiles = []\n",
    "        \n",
    "        with open('data\\profiles\\profiles2024-5.csv', 'r', encoding='utf-8') as file:\n",
    "            reader = csv.reader(file)\n",
    "            profiles = list(reader)\n",
    "\n",
    "\n",
    "        with open('data\\output2024-05-09210054.csv', 'r', encoding='utf-8') as file:\n",
    "            reader = csv.reader(file)\n",
    "            data = list(reader)\n",
    "            print(data[0])\n",
    "            for row in data[1:3]:\n",
    "                print(row)\n",
    "                booler = checkProfile(row[4],profiles)\n",
    "                if not booler:\n",
    "                    self.start_urls.append(self.base+(row[4]))\n",
    "\n",
    "    \n",
    "    def parse(self, response):\n",
    "        time.sleep(10)\n",
    "        # Extract data from the new CSS selector\n",
    "        for element in response.css('div.YouTubeUserTopInfo:nth-child(3) > span:nth-child(3)'):\n",
    "            subscribers = element.css('::text').extract_first().strip()\n",
    "            \n",
    "            subscribers = convert_to_number(subscribers.strip())\n",
    "        try:\n",
    "            for title in response.css('#socialblade-user-content > div:nth-child(3) > div'):\n",
    "                text=  title.css('::text').extract()\n",
    "                data= []\n",
    "                text = extract_values(text)\n",
    "                text[0] = text[0].strip()  # Remove leading and trailing whitespace\n",
    "                newsubscribers = convert_to_number(text[0])\n",
    "                color= title.css('sup>span::attr(style)').extract()\n",
    "                currentAccount= response.request.meta['redirect_urls'][0][len(self.base):]\n",
    "                text[1]= text[1][:-1]\n",
    "                if color[0] == \"color:#e53b00;\":\n",
    "                    text[1]='-'+text[1]\n",
    "                    print(currentAccount,subscribers,newsubscribers,text[1])\n",
    "                    data.append((currentAccount,subscribers,newsubscribers,text[1]))\n",
    "                else:\n",
    "                    data.append((currentAccount,subscribers,newsubscribers,text[1]))\n",
    "                yield self.addProfile([currentAccount,subscribers,newsubscribers,text[1]])\n",
    "        except:\n",
    "            currentAccount= response.request.meta['redirect_urls'][0][len(self.base):]\n",
    "            yield self.addProfile([currentAccount,subscribers,0,0])\n",
    "    \n",
    "    \n",
    "    def addProfile(self, profile):\n",
    "        with open('data\\profiles\\profiles2024-5.csv', 'a',newline='', encoding='utf-8') as file:\n",
    "            writer = csv.writer(file)\n",
    "            writer.writerow([profile[0],profile[1],profile[2],profile[3]])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "c571d5d7-61e7-416d-b79d-332194d8a26f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    process = CrawlerProcess({\n",
    "        'USER_AGENT': 'Mozilla/4.0 (compatible; MSIE 7.0; Windows NT 5.1)'\n",
    "    })\n",
    "\n",
    "    process.crawl(BlogSpider)\n",
    "    process.start() # the script will block here until the crawling is finished\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3f90b31-9097-485e-bb28-0d015feacf1c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
